{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e45ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé : 9564 observations, 49 colonnes\n",
      "\n",
      "Distribution de la cible :\n",
      "koi_disposition\n",
      "FALSE POSITIVE    4839\n",
      "CONFIRMED         2746\n",
      "CANDIDATE         1979\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Valeurs manquantes par colonne :\n",
      "koi_disposition       0\n",
      "koi_score          1510\n",
      "koi_period            0\n",
      "koi_duration          0\n",
      "koi_depth           363\n",
      "koi_model_snr       363\n",
      "koi_prad            363\n",
      "koi_teq             363\n",
      "koi_fpflag_nt         0\n",
      "koi_fpflag_ss         0\n",
      "dtype: int64\n",
      "\n",
      "Dataset nettoyé : 9564 observations\n",
      "\n",
      "Features utilisées : ['koi_score', 'koi_period', 'koi_duration', 'koi_depth', 'koi_model_snr', 'koi_prad', 'koi_teq', 'koi_fpflag_nt', 'koi_fpflag_ss', 'radius_period_ratio', 'fp_flag_sum', 'log_period', 'log_depth', 'log_snr']\n",
      "\n",
      "Classes encodées :\n",
      "  CANDIDATE -> 0\n",
      "  CONFIRMED -> 1\n",
      "  FALSE POSITIVE -> 2\n",
      "\n",
      "Train set : 7651 observations\n",
      "Test set  : 1913 observations\n",
      "\n",
      "======================================================================\n",
      "ENTRAÎNEMENT ET ÉVALUATION DES MODÈLES\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Modèle : Logistic Regression\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n",
      "/tmp/ipykernel_530752/497619568.py:61: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean[col].fillna(median_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  : 0.8881\n",
      "Precision : 0.8866\n",
      "Recall    : 0.8881\n",
      "F1-Score  : 0.8869\n",
      "CV Score  : 0.8791 (±0.0017)\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Modèle : Decision Tree\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Accuracy  : 0.8589\n",
      "Precision : 0.8604\n",
      "Recall    : 0.8589\n",
      "F1-Score  : 0.8594\n",
      "CV Score  : 0.8581 (±0.0050)\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Modèle : Random Forest\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Accuracy  : 0.9085\n",
      "Precision : 0.9088\n",
      "Recall    : 0.9085\n",
      "F1-Score  : 0.9086\n",
      "CV Score  : 0.9000 (±0.0028)\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Modèle : Gradient Boosting\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Accuracy  : 0.9064\n",
      "Precision : 0.9070\n",
      "Recall    : 0.9064\n",
      "F1-Score  : 0.9067\n",
      "CV Score  : 0.9012 (±0.0041)\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Modèle : SVM\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Accuracy  : 0.9022\n",
      "Precision : 0.9024\n",
      "Recall    : 0.9022\n",
      "F1-Score  : 0.9023\n",
      "CV Score  : 0.8943 (±0.0047)\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING - RANDOM FOREST\n",
      "======================================================================\n",
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "\n",
      "Meilleurs hyperparamètres :\n",
      "{'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "\n",
      "Performances du modèle optimisé :\n",
      "Accuracy  : 0.9080\n",
      "Precision : 0.9079\n",
      "Recall    : 0.9080\n",
      "F1-Score  : 0.9079\n",
      "\n",
      "======================================================================\n",
      "RAPPORT DE CLASSIFICATION - MEILLEUR MODÈLE\n",
      "======================================================================\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CANDIDATE       0.79      0.79      0.79       396\n",
      "     CONFIRMED       0.89      0.89      0.89       549\n",
      "FALSE POSITIVE       0.97      0.97      0.97       968\n",
      "\n",
      "      accuracy                           0.91      1913\n",
      "     macro avg       0.88      0.88      0.88      1913\n",
      "  weighted avg       0.91      0.91      0.91      1913\n",
      "\n",
      "\n",
      "Matrice de confusion :\n",
      "[[312  59  25]\n",
      " [ 57 486   6]\n",
      " [ 26   3 939]]\n",
      "\n",
      "======================================================================\n",
      "IMPORTANCE DES FEATURES\n",
      "======================================================================\n",
      "            feature  importance\n",
      "          koi_score    0.309108\n",
      "        fp_flag_sum    0.162532\n",
      "      koi_model_snr    0.076823\n",
      "            log_snr    0.076302\n",
      "radius_period_ratio    0.060826\n",
      "      koi_fpflag_ss    0.050486\n",
      "      koi_fpflag_nt    0.047064\n",
      "           koi_prad    0.044082\n",
      "         koi_period    0.038612\n",
      "         log_period    0.033908\n",
      "            koi_teq    0.027470\n",
      "       koi_duration    0.025357\n",
      "          log_depth    0.024615\n",
      "          koi_depth    0.022815\n",
      "\n",
      "======================================================================\n",
      "MODÈLE SAUVEGARDÉ\n",
      "======================================================================\n",
      "Fichiers créés :\n",
      "  - best_exoplanet_model.pkl\n",
      "  - exoplanet_scaler.pkl\n",
      "  - exoplanet_label_encoder.pkl\n",
      "\n",
      "======================================================================\n",
      "RÉSUMÉ COMPARATIF DES MODÈLES\n",
      "======================================================================\n",
      "                     Accuracy  Precision    Recall  F1-Score   CV Mean    CV Std\n",
      "Logistic Regression  0.888134   0.886614  0.888134  0.886918  0.879101  0.001692\n",
      "Decision Tree        0.858860   0.860387  0.858860  0.859360  0.858059  0.004986\n",
      "Random Forest        0.908521   0.908779  0.908521  0.908644  0.900013  0.002822\n",
      "Gradient Boosting    0.906430   0.906980  0.906430  0.906692  0.901190  0.004098\n",
      "SVM                  0.902248   0.902384  0.902248  0.902313  0.894262  0.004706\n",
      "Random Forest Tuned  0.907998   0.907862  0.907998  0.907929  0.902497  0.000000\n",
      "\n",
      "Résultats sauvegardés dans : model_comparison_results.csv\n",
      "\n",
      "======================================================================\n",
      "EXEMPLE DE PRÉDICTION\n",
      "======================================================================\n",
      "\n",
      "Données d'entrée :\n",
      "      koi_score  koi_period  koi_duration  koi_depth  koi_model_snr  koi_prad  koi_teq  koi_fpflag_nt  koi_fpflag_ss  radius_period_ratio  fp_flag_sum  log_period  log_depth   log_snr\n",
      "4894        0.0  117.681378       12.7591    45362.0         2426.9     45.43    575.0              0              1             4.187828            1    4.776442  10.722452  7.794782\n",
      "\n",
      "Prédiction : FALSE POSITIVE\n",
      "\n",
      "Probabilités par classe :\n",
      "  CANDIDATE: 0.0000\n",
      "  CONFIRMED: 0.0000\n",
      "  FALSE POSITIVE: 1.0000\n",
      "\n",
      "======================================================================\n",
      "ANALYSE TERMINÉE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CHARGEMENT ET PRÉPARATION DES DONNÉES\n",
    "# ============================================================================\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv(\"datas.csv\")\n",
    "\n",
    "print(f\"Dataset chargé : {df.shape[0]} observations, {df.shape[1]} colonnes\")\n",
    "print(f\"\\nDistribution de la cible :\")\n",
    "print(df['koi_disposition'].value_counts())\n",
    "\n",
    "# Sélection des colonnes pertinentes\n",
    "selected_columns = [\n",
    "    'koi_disposition',      # CIBLE (Confirmed / Candidate / False Positive)\n",
    "    'koi_score',            # score de confiance du transit\n",
    "    'koi_period',           # période orbitale [jours]\n",
    "    'koi_duration',         # durée du transit [heures]\n",
    "    'koi_depth',            # profondeur du transit [ppm]\n",
    "    'koi_model_snr',        # signal-to-noise du transit\n",
    "    'koi_prad',             # rayon planétaire [R_⊕]\n",
    "    'koi_teq',              # température d'équilibre [K]\n",
    "    'koi_fpflag_nt',        # transit non ressemblant (faux positif)\n",
    "    'koi_fpflag_ss'         # éclipse stellaire (faux positif)\n",
    "]\n",
    "\n",
    "df_clean = df[selected_columns].copy()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. NETTOYAGE DES DONNÉES\n",
    "# ============================================================================\n",
    "\n",
    "# Afficher les valeurs manquantes\n",
    "print(\"\\nValeurs manquantes par colonne :\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Remplir les valeurs manquantes numériques par la médiane\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if col != 'koi_disposition':\n",
    "        median_value = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_value, inplace=True)\n",
    "\n",
    "# Supprimer les lignes où la cible est manquante\n",
    "df_clean = df_clean.dropna(subset=['koi_disposition'])\n",
    "\n",
    "print(f\"\\nDataset nettoyé : {df_clean.shape[0]} observations\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FEATURES ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "# Créer des features supplémentaires basées sur les relations physiques\n",
    "\n",
    "# Ratio rayon/période (indicateur de densité orbitale)\n",
    "df_clean['radius_period_ratio'] = df_clean['koi_prad'] / np.sqrt(df_clean['koi_period'])\n",
    "\n",
    "# Score de faux positif composite\n",
    "df_clean['fp_flag_sum'] = df_clean['koi_fpflag_nt'] + df_clean['koi_fpflag_ss']\n",
    "\n",
    "# Logarithme de certaines features pour normaliser leur distribution\n",
    "df_clean['log_period'] = np.log1p(df_clean['koi_period'])\n",
    "df_clean['log_depth'] = np.log1p(df_clean['koi_depth'])\n",
    "df_clean['log_snr'] = np.log1p(df_clean['koi_model_snr'])\n",
    "\n",
    "# ============================================================================\n",
    "# 4. SÉPARATION FEATURES / TARGET\n",
    "# ============================================================================\n",
    "\n",
    "# Variable cible\n",
    "target = 'koi_disposition'\n",
    "y = df_clean[target]\n",
    "\n",
    "# Features (exclure la cible)\n",
    "X = df_clean.drop(columns=[target])\n",
    "\n",
    "print(f\"\\nFeatures utilisées : {list(X.columns)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENCODAGE DE LA CIBLE\n",
    "# ============================================================================\n",
    "\n",
    "# Encoder la variable cible (classification multi-classe)\n",
    "label_encoder_target = LabelEncoder()\n",
    "y_encoded = label_encoder_target.fit_transform(y)\n",
    "\n",
    "print(f\"\\nClasses encodées :\")\n",
    "for i, label in enumerate(label_encoder_target.classes_):\n",
    "    print(f\"  {label} -> {i}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SPLIT TRAIN/TEST\n",
    "# ============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set : {X_train.shape[0]} observations\")\n",
    "print(f\"Test set  : {X_test.shape[0]} observations\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. STANDARDISATION\n",
    "# ============================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============================================================================\n",
    "# 8. ENTRAÎNEMENT DES MODÈLES\n",
    "# ============================================================================\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRAÎNEMENT ET ÉVALUATION DES MODÈLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"Modèle : {name}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    \n",
    "    # Entraînement\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"CV Mean\": cv_mean,\n",
    "        \"CV Std\": cv_std\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy  : {accuracy:.4f}\")\n",
    "    print(f\"Precision : {precision:.4f}\")\n",
    "    print(f\"Recall    : {recall:.4f}\")\n",
    "    print(f\"F1-Score  : {f1:.4f}\")\n",
    "    print(f\"CV Score  : {cv_mean:.4f} (±{cv_std:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. HYPERPARAMETER TUNING (RANDOM FOREST)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING - RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [10, 20, 30, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Métriques du meilleur modèle\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "precision_best = precision_score(y_test, y_pred_best, average='weighted', zero_division=0)\n",
    "recall_best = recall_score(y_test, y_pred_best, average='weighted', zero_division=0)\n",
    "f1_best = f1_score(y_test, y_pred_best, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nMeilleurs hyperparamètres :\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nPerformances du modèle optimisé :\")\n",
    "print(f\"Accuracy  : {accuracy_best:.4f}\")\n",
    "print(f\"Precision : {precision_best:.4f}\")\n",
    "print(f\"Recall    : {recall_best:.4f}\")\n",
    "print(f\"F1-Score  : {f1_best:.4f}\")\n",
    "\n",
    "results[\"Random Forest Tuned\"] = {\n",
    "    \"Accuracy\": accuracy_best,\n",
    "    \"Precision\": precision_best,\n",
    "    \"Recall\": recall_best,\n",
    "    \"F1-Score\": f1_best,\n",
    "    \"CV Mean\": grid_search.best_score_,\n",
    "    \"CV Std\": 0\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# 10. RAPPORT DE CLASSIFICATION DÉTAILLÉ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAPPORT DE CLASSIFICATION - MEILLEUR MODÈLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\" + classification_report(\n",
    "    y_test, \n",
    "    y_pred_best, \n",
    "    target_names=label_encoder_target.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(cm)\n",
    "\n",
    "# ============================================================================\n",
    "# 11. IMPORTANCE DES FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPORTANCE DES FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# ============================================================================\n",
    "# 12. SAUVEGARDE DU MODÈLE\n",
    "# ============================================================================\n",
    "\n",
    "joblib.dump(best_model, \"best_exoplanet_model.pkl\")\n",
    "joblib.dump(scaler, \"exoplanet_scaler.pkl\")\n",
    "joblib.dump(label_encoder_target, \"exoplanet_label_encoder.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODÈLE SAUVEGARDÉ\")\n",
    "print(\"=\"*70)\n",
    "print(\"Fichiers créés :\")\n",
    "print(\"  - best_exoplanet_model.pkl\")\n",
    "print(\"  - exoplanet_scaler.pkl\")\n",
    "print(\"  - exoplanet_label_encoder.pkl\")\n",
    "\n",
    "# ============================================================================\n",
    "# 13. RÉSUMÉ COMPARATIF\n",
    "# ============================================================================\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RÉSUMÉ COMPARATIF DES MODÈLES\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Sauvegarder les résultats\n",
    "results_df.to_csv(\"model_comparison_results.csv\")\n",
    "print(\"\\nRésultats sauvegardés dans : model_comparison_results.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# 14. EXEMPLE DE PRÉDICTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXEMPLE DE PRÉDICTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "loaded_model = joblib.load(\"best_exoplanet_model.pkl\")\n",
    "loaded_scaler = joblib.load(\"exoplanet_scaler.pkl\")\n",
    "loaded_encoder = joblib.load(\"exoplanet_label_encoder.pkl\")\n",
    "\n",
    "# Exemple de données (première ligne du test set)\n",
    "example = X_test.iloc[0:1]\n",
    "example_scaled = loaded_scaler.transform(example)\n",
    "prediction = loaded_model.predict(example_scaled)\n",
    "prediction_proba = loaded_model.predict_proba(example_scaled)\n",
    "\n",
    "print(\"\\nDonnées d'entrée :\")\n",
    "print(example.to_string())\n",
    "print(f\"\\nPrédiction : {loaded_encoder.inverse_transform(prediction)[0]}\")\n",
    "print(\"\\nProbabilités par classe :\")\n",
    "for i, class_name in enumerate(loaded_encoder.classes_):\n",
    "    print(f\"  {class_name}: {prediction_proba[0][i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSE TERMINÉE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75b72e-b38d-4f9c-9f75-87b97589ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
